"""
Flight Delay Predictor - Classe compl√®te pour la pr√©diction des retards de vol
Refactoris√© √† partir du notebook machine_learning4.ipynb
Date: 27 septembre 2025
"""

import pandas as pd
import numpy as np
import warnings
import joblib
import json
from datetime import datetime, timedelta
from typing import Tuple, Dict, List, Optional, Union
from pathlib import Path

# Sklearn - Mod√®les
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.preprocessing import StandardScaler, RobustScaler, OneHotEncoder, OrdinalEncoder
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import (
    classification_report, confusion_matrix, roc_auc_score, 
    precision_recall_curve, roc_curve, average_precision_score,
    f1_score, precision_score, recall_score
)

# Mod√®les de Machine Learning
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier

# Imbalanced-learn
from imblearn.combine import SMOTEENN
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import EditedNearestNeighbours

# XGBoost et LightGBM
import xgboost as xgb
from xgboost import XGBClassifier

# LightGBM (optionnel)
try:
    import lightgbm as lgb
    from lightgbm import LGBMClassifier
    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False
    print("‚ö†Ô∏è LightGBM non install√©. Utilisez: pip install lightgbm")

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns

warnings.filterwarnings('ignore')


class FlightDelayPredictor:
    """
    Classe compl√®te pour pr√©dire les retards de vol en utilisant des donn√©es m√©t√©orologiques
    et temporelles optimis√©es.
    
    Mod√®les support√©s:
    - decision_tree: Arbre de d√©cision
    - random_forest: For√™t al√©atoire  
    - logistic_regression: R√©gression logistique
    - svm: Machine √† vecteurs de support
    - knn: K plus proches voisins
    - xgboost: XGBoost standard
    - xgboost_tuned: XGBoost optimis√© (recommand√©)
    - lightgbm: LightGBM (si install√©)
    """
    
    def __init__(self, 
                 delay_threshold: int = 15,
                 sample_size: Optional[int] = None,
                 random_state: int = 42,
                 output_dir: str = "machine learning/model_output"):
        """
        Initialise le pr√©dicteur de retards de vol.
        
        Args:
            delay_threshold: Seuil en minutes pour consid√©rer un vol en retard
            sample_size: Taille d'√©chantillon pour l'entra√Ænement (None = toutes les donn√©es)
            random_state: Graine al√©atoire pour la reproductibilit√©
            output_dir: R√©pertoire de sortie pour sauvegarder les mod√®les
        """
        self.delay_threshold = delay_threshold
        self.sample_size = sample_size
        self.random_state = random_state
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # Configuration des caract√©ristiques
        self.numeric_features = [
            # M√©t√©o de base (vent, directions)
            'wind_speed_kt', 'wind_dir_degrees', 'wind_gust_kt', 
            't_wind_speed_kt', 't_wind_dir_degrees', 't_wind_gust_kt', 

            'departure_delay_minutes',
            'flight_duration_minutes',
            'airport_flight_count',
            'airline_flight_count',
            
            # Scores m√©t√©o calcul√©s
            'weather_severity_dep', 'weather_severity_arr',
            
            # Temps et dur√©e
            'flight_duration_hours',
            
            # Caract√©ristiques temporelles
            'departure_hour_local', 'arrival_hour_local',
            'departure_dayofweek', 'arrival_dayofweek',
            'departure_month', 'departure_quarter', 'departure_day',
            
            # Indicateurs binaires temporels
            'is_rush_hour_dep', 'is_rush_hour_arr', 'is_weekend',
            'is_month_end', 'is_month_start',
            
            # Indicateurs m√©t√©o binaires (optimis√©s)
            'dep_has_convective', 'dep_has_icing', 'dep_visibility_affected', 'dep_wind_affected',
            'arr_has_convective', 'arr_has_icing', 'arr_visibility_affected', 'arr_wind_affected'
        ]
        
        self.categorical_features = [
            # A√©roports et compagnies
            # 'airline_code', 'from_airport', 'to_airport',
            
            # Codes m√©t√©o simplifi√©s (optimis√©s)
            'dep_weather_simplified', 'arr_weather_simplified',
            
            # Niveaux d'impact m√©t√©o
            'dep_weather_impact', 'arr_weather_impact', 'overall_weather_impact'
        ]
        
        self.ordered_features = [
            'visibility_statute_mi', 't_visibility_statute_mi',     
            'msc_sky_cover', 'tsc_sky_cover'
        ]
        
        # Ordres pour l'encodage ordinal
        self.visibility_order = ['<1', '<2', '<3', '<4', '<5', '>=5']
        self.sky_cover_order = ['SKC', 'CAVOK', 'CLR', 'OVX', 'FEW', 'SCT', 'BKN', 'OVC']
        
        # Attributs initialis√©s lors de l'entra√Ænement
        self.preprocessor = None
        self.model = None
        self.optimal_threshold = None
        self.class_weights = None
        self.feature_importance = None
        self.training_metrics = {}
    
    @staticmethod
    def get_available_models() -> Dict[str, str]:
        """
        Retourne la liste des mod√®les disponibles avec leurs descriptions
        
        Returns:
            Dictionnaire {nom_modele: description}
        """
        base_models = {
            'decision_tree': 'Arbre de d√©cision - Simple et interpr√©table',
            'random_forest': 'For√™t al√©atoire - Robuste, bon par d√©faut',
            'logistic_regression': 'R√©gression logistique - Rapide et lin√©aire',
            'svm': 'Machine √† vecteurs de support - Puissant pour donn√©es complexes',
            'knn': 'K plus proches voisins - Simple, bas√© sur la similarit√©',
            'xgboost': 'XGBoost standard - Gradient boosting performant',
            'xgboost_tuned': 'XGBoost optimis√© - Recommand√© pour classes d√©s√©quilibr√©es'
        }
        
        if LIGHTGBM_AVAILABLE:
            base_models['lightgbm'] = 'LightGBM - Alternative rapide √† XGBoost'
        
        return base_models
    
    @staticmethod
    def print_available_models():
        """Affiche la liste des mod√®les disponibles"""
        models = FlightDelayPredictor.get_available_models()
        
        print("ü§ñ MOD√àLES DE ML DISPONIBLES:")
        print("=" * 50)
        
        for model_name, description in models.items():
            status = "‚úÖ" if model_name != 'lightgbm' or LIGHTGBM_AVAILABLE else "‚ùå"
            print(f"{status} {model_name:18} : {description}")
        
        if not LIGHTGBM_AVAILABLE:
            print(f"\nüí° Pour activer LightGBM: pip install lightgbm")
        
        print(f"\nüéØ Recommand√©: 'xgboost_tuned' pour classes d√©s√©quilibr√©es")
        
    def load_and_prepare_data(self, data_path: str, airports_ref_path: str, 
                              for_training: bool = True) -> pd.DataFrame:
        """
        Charge et pr√©pare les donn√©es avec toutes les transformations n√©cessaires.
        
        Args:
            data_path: Chemin vers le fichier de donn√©es principal
            airports_ref_path: Chemin vers le fichier de r√©f√©rence des a√©roports
            for_training: Si True, applique les filtres d'entra√Ænement (nettoyage, filtrage temporel)
                         Si False, mode production sans filtres
            
        Returns:
            DataFrame pr√©par√© avec toutes les caract√©ristiques
        """
        
        # Chargement des donn√©es
        df = pd.read_csv(data_path)
        airports_ref = pd.read_csv(airports_ref_path, sep=';')[['code_iata', 'timezone']]
        
        print(f"‚úÖ Donn√©es charg√©es: {len(df):,} lignes")
        
        # Nettoyage des trous uniquement pour l'entra√Ænement
        if for_training:
            df = self._remove_data_gaps(df)
            print(f"‚úÖ Donn√©es apr√®s nettoyage des trous: {len(df):,} lignes")
        
        # Pr√©paration des donn√©es avec ou sans filtres selon le mode
        df = self._prepare_base_features(df, airports_ref, for_training=for_training)
        df = self._create_weather_features(df)
        df = self._create_temporal_features(df)
        
        # Cr√©ation de la variable cible uniquement pour l'entra√Ænement
        if for_training:
            df = self._create_target_variable(df)
            
            # √âchantillonnage si n√©cessaire
            if self.sample_size and len(df) > self.sample_size:
                df = df.sample(n=self.sample_size, random_state=self.random_state)
                print(f"üìä √âchantillonnage: {len(df):,} lignes conserv√©es")
        
        print("‚úÖ Pr√©paration des donn√©es termin√©e")
        return df
    
    def _remove_data_gaps(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Supprime les vols d'une heure compl√®te si TOUS les vols de cette heure
        ont un status_final manquant (NaN, vide, ou null).
        
        Args:
            df: DataFrame avec les donn√©es de vol
            
        Returns:
            DataFrame nettoy√© sans les heures o√π tous les status_final sont manquants
        """
        if 'departure_scheduled_utc' not in df.columns:
            print("‚ö†Ô∏è Colonne departure_scheduled_utc non trouv√©e, pas de nettoyage")
            return df
            
        if 'status_final' not in df.columns:
            print("‚ö†Ô∏è Colonne status_final non trouv√©e, pas de nettoyage")
            return df
        
        # Convertir en datetime et trier
        df = df.copy()
        df['departure_scheduled_utc'] = pd.to_datetime(df['departure_scheduled_utc'], errors='coerce')
        
        # Supprimer les lignes avec des dates invalides
        initial_count = len(df)
        df = df.dropna(subset=['departure_scheduled_utc'])
        if len(df) < initial_count:
            print(f"‚ö†Ô∏è {initial_count - len(df)} lignes supprim√©es (dates invalides)")
        
        # Cr√©er une colonne d'heure pour grouper
        df['departure_hour'] = df['departure_scheduled_utc'].dt.floor('H')
        
        # Analyser chaque heure
        hours_to_remove = []
        total_flights_to_remove = 0
        
        for hour, group in df.groupby('departure_hour'):
            # V√©rifier si tous les status_final de cette heure sont manquants
            status_series = group['status_final']
            
            # Compter les status_final valides (non NaN, non vides, non null)
            valid_status = status_series.dropna()  # Supprime les NaN
            valid_status = valid_status[valid_status.astype(str).str.strip() != '']  # Supprime les vides
            valid_status = valid_status[valid_status.astype(str).str.lower() != 'null']  # Supprime les 'null'
            
            total_flights_in_hour = len(group)
            valid_status_count = len(valid_status)
            
            if valid_status_count == 0:
                # TOUS les status_final sont manquants pour cette heure
                hours_to_remove.append(hour)
                total_flights_to_remove += total_flights_in_hour
                print(f"  ÔøΩÔ∏è Heure {hour}: {total_flights_in_hour} vols - TOUS status_final manquants")
            else:
                print(f"  ‚úÖ Heure {hour}: {total_flights_in_hour} vols - {valid_status_count} status_final valides")
        
        # Supprimer les heures identifi√©es
        if hours_to_remove:
            print(f"üßπ Suppression de {len(hours_to_remove)} heures compl√®tes avec status_final manquants:")
            for hour in hours_to_remove:
                flights_in_hour = len(df[df['departure_hour'] == hour])
                print(f"  üìÖ {hour} - {flights_in_hour} vols supprim√©s")
            
            # Cr√©er le masque pour garder les lignes
            mask_to_keep = ~df['departure_hour'].isin(hours_to_remove)
            df_cleaned = df[mask_to_keep].reset_index(drop=True)
            
            print(f"üìä Total: {total_flights_to_remove} vols supprim√©s")
        else:
            df_cleaned = df
        
        # Nettoyer la colonne temporaire
        return df_cleaned.drop('departure_hour', axis=1)
    
    def _prepare_base_features(self, df: pd.DataFrame, airports_ref: pd.DataFrame, 
                               for_training: bool = True) -> pd.DataFrame:
        """
        Pr√©pare les caract√©ristiques de base (colonnes, fuseaux horaires, etc.)
        
        Args:
            df: DataFrame avec les donn√©es brutes
            airports_ref: DataFrame de r√©f√©rence des a√©roports
            for_training: Si True, applique les filtres d'entra√Ænement
        """
        
        # S√©lection des colonnes essentielles
        colonnes_a_garder = [
            'airline_code', 
            'from_airport', 
            'to_airport', 
            'status',
            'status_final', 
            'delay_min',
            'departure_scheduled_utc', 
            'departure_actual_utc',
            'arrival_scheduled_utc',
            'wind_speed_kt', 
            'wind_dir_degrees', 
            'wind_gust_kt',
            'visibility_statute_mi', 
            'msc_sky_cover', 'wx_string',
            't_wind_speed_kt', 
            't_wind_dir_degrees', 
            't_wind_gust_kt',
            't_visibility_statute_mi', 
            'tsc_sky_cover', 
            't_wx_string'
        ]
        
        df = df[colonnes_a_garder].copy()
        
        # Conversion de TOUTES les colonnes datetime
        datetime_columns = ['departure_scheduled_utc', 'departure_actual_utc', 'arrival_scheduled_utc']
        for col in datetime_columns:
            if col in df.columns:
                df[col] = pd.to_datetime(df[col], errors='coerce')
        
        # Conversion des colonnes num√©riques (pour √©viter les erreurs de type)
        numeric_cols = ['visibility_statute_mi', 't_visibility_statute_mi', 
                       'wind_speed_kt', 'wind_gust_kt', 't_wind_speed_kt', 't_wind_gust_kt']
        for col in numeric_cols:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
        
        # Filtrage temporel UNIQUEMENT pour l'entra√Ænement. 
        # On n'entraine pas le mod√®le sur des donn√©es dont on a pas encore r√©cup√©r√© le statut final.
        if for_training:
            date_max = df['departure_scheduled_utc'].max()
            date_seuil = date_max - pd.Timedelta(hours=24)
            df = df[df['departure_scheduled_utc'] < date_seuil]
            print(f"‚úÖ Filtrage temporel: {len(df):,} lignes conserv√©es (< {date_seuil})")
        
        # Ajout des fuseaux horaires
        timezone_dict = airports_ref.set_index('code_iata')['timezone'].to_dict()
        df['departure_timezone'] = df['from_airport'].map(timezone_dict)
        df['arrival_timezone'] = df['to_airport'].map(timezone_dict)

        # Ajout du nombre de minute de retard au d√©part
        df['departure_delay_minutes'] = (df['departure_actual_utc'] - df['departure_scheduled_utc']).dt.total_seconds() / 60
        
        # Ajout de la dur√©e du vol
        df['flight_duration_minutes'] = (df['arrival_scheduled_utc'] - df['departure_scheduled_utc']).dt.total_seconds() / 60

        # Ajout de l'importance de l'a√©roport bas√©e sur le nombre de vols
        df['airport_flight_count'] = df['from_airport'].map(df['from_airport'].value_counts(normalize=True))

        # Ajout de l'importance de la compagnie a√©rienne bas√©e sur le nombre de vols
        df['airline_flight_count'] = df['airline_code'].map(df['airline_code'].value_counts(normalize=True))

        # Suppression des status CANCELLED UNIQUEMENT pour l'entra√Ænement
        if for_training:
            df = df[df['status'] != 'CANCELLED']
            print(f"‚úÖ Status CANCELLED exclus: {len(df):,} lignes conserv√©es")

        return df
    
    def _create_weather_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Cr√©e les caract√©ristiques m√©t√©orologiques optimis√©es"""
        
        # Gestion des valeurs manquantes
        weather_cols = ['wind_speed_kt', 't_wind_speed_kt', 'wind_gust_kt', 't_wind_gust_kt']
        for col in weather_cols:
            if col in df.columns:
                df[col] = df[col].fillna(0)
        
        # Classification de la visibilit√©
        def visibility_to_class(vis):
            if pd.isna(vis):
                return np.nan
            elif vis < 1: return '<1'
            elif vis < 2: return '<2'
            elif vis < 3: return '<3'
            elif vis < 4: return '<4'
            elif vis < 5: return '<5'
            else: return '>=5'
        
        df['visibility_statute_mi'] = df['visibility_statute_mi'].apply(visibility_to_class)
        df['t_visibility_statute_mi'] = df['t_visibility_statute_mi'].apply(visibility_to_class)
        
        # Scores de s√©v√©rit√© m√©t√©o
        df['weather_severity_dep'] = (
            (df['wind_speed_kt'] > 20).astype(int) + 
            (df['wind_gust_kt'] > 30).astype(int) +
            (df['visibility_statute_mi'].isin(['<1', '<2'])).astype(int)
        )
        
        df['weather_severity_arr'] = (
            (df['t_wind_speed_kt'] > 20).astype(int) + 
            (df['t_wind_gust_kt'] > 30).astype(int) +
            (df['t_visibility_statute_mi'].isin(['<1', '<2'])).astype(int)
        )
        
        # Traitement intelligent des codes wx_string
        df = self._process_weather_codes(df)
        
        return df
    
    def _process_weather_codes(self, df: pd.DataFrame) -> pd.DataFrame:
        """Traite les codes m√©t√©orologiques METAR en cat√©gories optimis√©es"""
        
        def extract_impactful_weather_codes(wx_string):
            """Extrait les codes m√©t√©o ayant un impact sur les retards"""
            if pd.isna(wx_string) or wx_string == '' or wx_string == 'nan':
                return {
                    'impact_level': 'none',
                    'simplified_code': 'CLEAR',
                    'convective': False,
                    'icing': False,
                    'visibility_impact': False,
                    'wind_impact': False
                }
            
            wx_str = str(wx_string).upper().strip()
            
            # D√©tection des ph√©nom√®nes critiques
            thunderstorms = 'TS' in wx_str
            hail = 'GR' in wx_str
            fog = 'FG' in wx_str
            freezing = 'FZ' in wx_str
            snow = 'SN' in wx_str
            rain = 'RA' in wx_str
            squalls = 'SQ' in wx_str
            dust_storm = any(x in wx_str for x in ['SS', 'DS'])
            
            # Classification par impact
            if thunderstorms or hail or squalls or dust_storm:
                impact_level = 'high'
                if thunderstorms: simplified_code = 'THUNDERSTORM'
                elif hail: simplified_code = 'HAIL'
                elif dust_storm: simplified_code = 'DUST_STORM'
                else: simplified_code = 'THUNDERSTORM'
            elif fog or (freezing and (rain or snow)):
                impact_level = 'high' if fog else 'medium'
                simplified_code = 'FOG' if fog else 'ICING'
            elif snow or (rain and ('HVY' in wx_str or '+' in wx_str)):
                impact_level = 'medium'
                simplified_code = 'SNOW' if snow else 'RAIN'
            elif rain or 'DZ' in wx_str or 'BR' in wx_str:
                impact_level = 'low'
                simplified_code = 'LIGHT_WEATHER'
            else:
                impact_level = 'none'
                simplified_code = 'CLEAR'
            
            return {
                'impact_level': impact_level,
                'simplified_code': simplified_code,
                'convective': thunderstorms or squalls or hail,
                'icing': freezing or 'IC' in wx_str or 'PE' in wx_str,
                'visibility_impact': fog or 'BR' in wx_str,
                'wind_impact': thunderstorms or squalls
            }
        
        # Application aux colonnes m√©t√©o
        for prefix, col in [('dep', 'wx_string'), ('arr', 't_wx_string')]:
            if col in df.columns:
                weather_info = df[col].apply(extract_impactful_weather_codes)
                
                df[f'{prefix}_weather_impact'] = [w['impact_level'] for w in weather_info]
                df[f'{prefix}_weather_simplified'] = [w['simplified_code'] for w in weather_info]
                df[f'{prefix}_has_convective'] = [w['convective'] for w in weather_info]
                df[f'{prefix}_has_icing'] = [w['icing'] for w in weather_info]
                df[f'{prefix}_visibility_affected'] = [w['visibility_impact'] for w in weather_info]
                df[f'{prefix}_wind_affected'] = [w['wind_impact'] for w in weather_info]
        
        # Impact m√©t√©orologique global
        df['overall_weather_impact'] = 'none'
        high_mask = (df['dep_weather_impact'] == 'high') | (df['arr_weather_impact'] == 'high')
        medium_mask = ((df['dep_weather_impact'] == 'medium') | (df['arr_weather_impact'] == 'medium')) & (~high_mask)
        low_mask = ((df['dep_weather_impact'] == 'low') | (df['arr_weather_impact'] == 'low')) & (~high_mask) & (~medium_mask)
        
        df.loc[high_mask, 'overall_weather_impact'] = 'high'
        df.loc[medium_mask, 'overall_weather_impact'] = 'medium'
        df.loc[low_mask, 'overall_weather_impact'] = 'low'
        
        return df
    
    def _create_temporal_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Cr√©e les caract√©ristiques temporelles avanc√©es"""
        
        # Conversion UTC vers heure locale
        dep_utc = pd.to_datetime(df['departure_scheduled_utc'], utc=True, errors='coerce')
        arr_utc = pd.to_datetime(df['arrival_scheduled_utc'], utc=True, errors='coerce')
        
        def convert_utc_grouped(utc_series, tz_series):
            """Conversion vectoris√©e UTC -> local par groupes de fuseaux"""
            out = pd.Series(pd.NaT, index=utc_series.index, dtype='datetime64[ns]')
            for tz in tz_series.dropna().unique():
                mask = tz_series == tz
                try:
                    out.loc[mask] = utc_series.loc[mask].dt.tz_convert(tz).dt.tz_localize(None)
                except Exception:
                    pass
            return out
        
        df['departure_scheduled_local'] = convert_utc_grouped(dep_utc, df['departure_timezone'])
        df['arrival_scheduled_local'] = convert_utc_grouped(arr_utc, df['arrival_timezone'])
        
        # Caract√©ristiques temporelles de base
        df['departure_hour_local'] = df['departure_scheduled_local'].dt.hour
        df['arrival_hour_local'] = df['arrival_scheduled_local'].dt.hour
        df['departure_dayofweek'] = df['departure_scheduled_local'].dt.dayofweek
        df['arrival_dayofweek'] = df['arrival_scheduled_local'].dt.dayofweek
        
        # Caract√©ristiques temporelles avanc√©es
        df['departure_month'] = df['departure_scheduled_local'].dt.month
        df['departure_quarter'] = df['departure_scheduled_local'].dt.quarter
        df['departure_day'] = df['departure_scheduled_local'].dt.day
        
        # Dur√©e du vol (utilise les colonnes d√©j√† converties)
        duration_seconds = (df['arrival_scheduled_utc'] - df['departure_scheduled_utc']).dt.total_seconds()
        df['flight_duration_hours'] = (duration_seconds / 3600).round(1)
        
        # Indicateurs temporels
        df['is_rush_hour_dep'] = df['departure_hour_local'].isin([7, 8, 17, 18, 19]).astype(int)
        df['is_rush_hour_arr'] = df['arrival_hour_local'].isin([7, 8, 17, 18, 19]).astype(int)
        df['is_weekend'] = (df['departure_dayofweek'] >= 5).astype(int)
        df['is_month_end'] = (df['departure_day'] > 25).astype(int)
        df['is_month_start'] = (df['departure_day'] <= 5).astype(int)
        
        return df
    
    def _create_target_variable(self, df: pd.DataFrame) -> pd.DataFrame:
        """Cr√©e la variable cible (retard/pas de retard)"""

        # Remplacer les Na par 0 (si on a pas l'info on part du principe qu'il n'y a pas de retard)
        df['delay_min'] = df['delay_min'].fillna(0)

        # Cr√©er la variable binaire de retard
        df['delay'] = (df['delay_min'] > self.delay_threshold).astype(int)
        
        print(f"üìä Distribution des retards (seuil {self.delay_threshold}min):")
        print(f"  Pas de retard: {(df['delay'] == 0).sum():,} ({(df['delay'] == 0).mean()*100:.1f}%)")
        print(f"  Retard: {(df['delay'] == 1).sum():,} ({(df['delay'] == 1).mean()*100:.1f}%)")
        
        return df
    
    def create_preprocessor(self) -> ColumnTransformer:
        """Cr√©e le pipeline de preprocessing optimis√©"""
        
        # Pipeline num√©rique
        numeric_transformer = Pipeline([
            ("imputer", SimpleImputer(missing_values=np.nan, strategy="median")),
            ("scaler", RobustScaler())
        ])
        
        # Pipeline cat√©goriel
        categorical_transformer = OneHotEncoder(
            drop="first", 
            handle_unknown="ignore", 
            sparse_output=False,
            max_categories=20
        )
        
        # Pipeline ordinal
        ordered_transformer = OrdinalEncoder(
            categories=[
                self.visibility_order, self.visibility_order,
                self.sky_cover_order, self.sky_cover_order
            ], 
            handle_unknown='use_encoded_value', 
            unknown_value=-1
        )
        
        # Assemblage du preprocesseur
        preprocessor = ColumnTransformer([
            ("num", numeric_transformer, self.numeric_features),
            ("cat", categorical_transformer, self.categorical_features),
            ("ord", ordered_transformer, self.ordered_features)
        ], 
        remainder='drop',
        n_jobs=-1
        )
        
        return preprocessor
    
    def balance_classes(self, X_train: np.ndarray, y_train: pd.Series) -> Tuple[np.ndarray, pd.Series]:
        """
        Applique SMOTEENN pour g√©rer le d√©s√©quilibre des classes
        
        Returns:
            Tuple des donn√©es r√©√©quilibr√©es (X_train_balanced, y_train_balanced)
        """
        print("üîÑ R√©√©quilibrage des classes avec SMOTEENN...")
        
        try:
            # Application de SMOTEENN (SMOTE + EditedNearestNeighbours)
            smoteenn = SMOTEENN(
                random_state=self.random_state,
                smote=SMOTE(random_state=self.random_state, k_neighbors=3),
                enn=EditedNearestNeighbours()
            )
            
            print(f"  Distribution avant: {dict(pd.Series(y_train).value_counts())}")
            X_balanced, y_balanced = smoteenn.fit_resample(X_train, y_train)
            print(f"  Distribution apr√®s: {dict(pd.Series(y_balanced).value_counts())}")
            print("  ‚úÖ SMOTEENN appliqu√© avec succ√®s")
            
            return X_balanced, y_balanced
            
        except Exception as e:
            print(f"  ‚ùå SMOTEENN √©chou√©: {e}")
            print("  ‚ö†Ô∏è Utilisation des donn√©es originales")
            return X_train, y_train
    
    def train(self, df: pd.DataFrame, model_type: str = 'xgboost_tuned') -> Dict:
        """
        Entra√Æne le mod√®le de pr√©diction des retards
        
        Args:
            df: DataFrame pr√©par√© avec toutes les caract√©ristiques
            model_type: Type de mod√®le √† utiliser
            
        Returns:
            Dictionnaire avec les m√©triques d'entra√Ænement
        """
        print(f"üöÄ D√©but de l'entra√Ænement du mod√®le {model_type}...")
        
        # Pr√©paration des donn√©es
        feature_cols = self.numeric_features + self.categorical_features + self.ordered_features
        existing_cols = [col for col in feature_cols if col in df.columns]
        
        X = df[existing_cols]
        y = df['delay']
        
        # Division train/test avec stratification
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.25, random_state=self.random_state, stratify=y
        )
        
        # Preprocessing
        self.preprocessor = self.create_preprocessor()
        
        # Mise √† jour des listes de caract√©ristiques avec les colonnes existantes
        self.numeric_features = [col for col in self.numeric_features if col in existing_cols]
        self.categorical_features = [col for col in self.categorical_features if col in existing_cols]
        self.ordered_features = [col for col in self.ordered_features if col in existing_cols]
        
        # Recr√©er le preprocesseur avec les colonnes existantes
        self.preprocessor = ColumnTransformer([
            ("num", Pipeline([("imputer", SimpleImputer(strategy="median")), ("scaler", RobustScaler())]), 
             self.numeric_features),
            ("cat", OneHotEncoder(drop="first", handle_unknown="ignore", sparse_output=False, max_categories=20), 
             self.categorical_features),
            ("ord", OrdinalEncoder(categories=[self.visibility_order, self.visibility_order, 
                                             self.sky_cover_order, self.sky_cover_order], 
                                 handle_unknown='use_encoded_value', unknown_value=-1), 
             self.ordered_features)
        ], remainder='drop', n_jobs=-1)
        
        X_train_trans = self.preprocessor.fit_transform(X_train)
        X_test_trans = self.preprocessor.transform(X_test)
        
        # R√©√©quilibrage des classes
        X_train_balanced, y_train_balanced = self.balance_classes(X_train_trans, y_train)
        
        # Calcul des poids de classe
        class_counts = np.bincount(y_train)
        self.class_weights = len(y_train) / (len(class_counts) * class_counts)
        
        # Cr√©ation du mod√®le selon le type choisi
        self.model = self._create_model(model_type, y_train)
        
        # Entra√Ænement
        print("  Entra√Ænement du mod√®le...")
        self.model.fit(X_train_balanced, y_train_balanced)
        
        # Pr√©dictions
        y_pred_proba = self.model.predict_proba(X_test_trans)[:, 1]
        
        # Optimisation du seuil
        self._optimize_threshold(y_test, y_pred_proba)
        
        # Calcul des seuils de risque automatiques bas√©s sur la distribution
        self._calculate_risk_thresholds(y_test, y_pred_proba)
        
        # Calcul des m√©triques
        metrics = self._calculate_metrics(y_test, y_pred_proba, X_test_trans)
        
        # Stocker les derni√®res pr√©dictions pour les graphiques
        self.last_y_true = y_test
        self.last_y_pred_proba = y_pred_proba
        
        # Importance des caract√©ristiques
        if hasattr(self.model, 'feature_importances_'):
            # Utiliser les vrais noms de features du preprocessor
            try:
                feature_names = self.preprocessor.get_feature_names_out()
                # Nettoyer les noms pour une meilleure lisibilit√©
                cleaned_names = []
                for name in feature_names:
                    if name.startswith('cat__'):
                        # Transformer cat__airline_code__AA en airline_code=AA
                        parts = name.split('__')
                        if len(parts) >= 3:
                            cleaned_name = f"{parts[1]}={parts[2]}"
                        else:
                            cleaned_name = name
                    elif name.startswith('num__'):
                        # Retirer le pr√©fixe num__
                        cleaned_name = name.replace('num__', '')
                    elif name.startswith('ord__'):
                        # Retirer le pr√©fixe ord__
                        cleaned_name = name.replace('ord__', '')
                    else:
                        cleaned_name = name
                    cleaned_names.append(cleaned_name)
                feature_names = cleaned_names
            except:
                # Fallback vers les noms g√©n√©riques si erreur
                feature_names = (self.numeric_features + 
                               [f"cat_{i}" for i in range(len(self.categorical_features) * 10)] + 
                               self.ordered_features)
            
            # Ajuster la longueur des noms de caract√©ristiques
            n_features = len(self.model.feature_importances_)
            if len(feature_names) > n_features:
                feature_names = feature_names[:n_features]
            elif len(feature_names) < n_features:
                feature_names.extend([f"feature_{i}" for i in range(len(feature_names), n_features)])
            
            self.feature_importance = pd.DataFrame({
                'feature': feature_names[:n_features],
                'importance': self.model.feature_importances_
            }).sort_values('importance', ascending=False)
        
        self.training_metrics = metrics
        
        # üîç D√âTECTION AUTOMATIQUE DE L'OVERFITTING
        overfitting_analysis = self.detect_overfitting(
            X_train_balanced, y_train_balanced, 
            X_test_trans, y_test
        )
        
        # Ajouter les r√©sultats d'overfitting aux m√©triques
        self.training_metrics['overfitting_analysis'] = overfitting_analysis
        
        print("‚úÖ Entra√Ænement termin√©!")
        
        return metrics
    
    def get_detailed_feature_names(self) -> Dict[str, str]:
        """
        Retourne un mapping entre les noms de features g√©n√©riques (cat_X) 
        et les vrais noms de colonnes apr√®s OneHot encoding.
        
        Returns:
            Dict[str, str]: Mapping feature_name -> real_column_name
        """
        if not hasattr(self, 'preprocessor') or self.preprocessor is None:
            print("‚ùå Le mod√®le doit √™tre entra√Æn√© avant de pouvoir obtenir les noms de features d√©taill√©s")
            return {}
        
        try:
            # Obtenir les noms de features du ColumnTransformer
            feature_names = self.preprocessor.get_feature_names_out()
            
            # Cr√©er un mapping d√©taill√©
            mapping = {}
            for i, name in enumerate(feature_names):
                generic_name = f"feature_{i}" if i >= len(self.numeric_features) + len(self.categorical_features) * 10 + len(self.ordered_features) else None
                
                if i < len(self.numeric_features):
                    # Features num√©riques
                    mapping[self.numeric_features[i]] = name
                elif i < len(self.numeric_features) + len(feature_names[len(self.numeric_features):]):
                    # Features cat√©gorielles et ordinales
                    cat_index = i - len(self.numeric_features)
                    generic_cat_name = f"cat_{cat_index}"
                    mapping[generic_cat_name] = name
            
            return mapping
            
        except Exception as e:
            print(f"‚ùå Erreur lors de la r√©cup√©ration des noms de features: {e}")
            return {}
    
    def explain_feature_importance(self, top_n: int = 20) -> pd.DataFrame:
        """
        Affiche l'importance des features avec leurs vrais noms de colonnes.
        
        Args:
            top_n: Nombre de features les plus importantes √† afficher
            
        Returns:
            DataFrame avec l'importance des features et leurs vrais noms
        """
        if not hasattr(self, 'feature_importance') or self.feature_importance is None:
            print("‚ùå Le mod√®le doit √™tre entra√Æn√© avant de pouvoir expliquer l'importance des features")
            return pd.DataFrame()
        
        # Obtenir le mapping des noms
        feature_mapping = self.get_detailed_feature_names()
        
        # Cr√©er une version enrichie du DataFrame d'importance
        detailed_importance = self.feature_importance.copy()
        detailed_importance['real_feature_name'] = detailed_importance['feature'].map(
            lambda x: feature_mapping.get(x, x)
        )
        
        # Afficher le top N
        top_features = detailed_importance.head(top_n)
        
        print(f"\nüéØ Top {top_n} des features les plus importantes:")
        print("=" * 80)
        for idx, row in top_features.iterrows():
            print(f"{row['feature']:15} -> {row['real_feature_name']:40} | Importance: {row['importance']:.4f}")
        
        return detailed_importance
    
    def show_readable_feature_importance(self, top_n: int = 20) -> None:
        """
        Affiche l'importance des features avec des descriptions en fran√ßais compr√©hensibles.
        
        Args:
            top_n: Nombre de features les plus importantes √† afficher
        """
        if not hasattr(self, 'feature_importance') or self.feature_importance is None:
            print("‚ùå Le mod√®le doit √™tre entra√Æn√© avant de pouvoir afficher l'importance des features")
            return
        
        # Dictionnaire de traduction pour rendre les noms plus compr√©hensibles
        descriptions = {
            # Features num√©riques - m√©t√©o
            'wind_speed_kt': 'üå™Ô∏è Vitesse du vent au d√©part (n≈ìuds)',
            'wind_gust_kt': 'üí® Rafales de vent au d√©part (n≈ìuds)',
            't_wind_speed_kt': 'üå™Ô∏è Vitesse du vent √† l\'arriv√©e (n≈ìuds)',
            't_wind_gust_kt': 'üí® Rafales de vent √† l\'arriv√©e (n≈ìuds)',
            'temperature_c': 'üå°Ô∏è Temp√©rature au d√©part (¬∞C)',
            't_temperature_c': 'üå°Ô∏è Temp√©rature √† l\'arriv√©e (¬∞C)',
            'humidity_percent': 'üíß Humidit√© au d√©part (%)',
            'pressure_altimeter_hg': 'üìä Pression atmosph√©rique d√©part',
            'visibility_statute_mi': 'üëÅÔ∏è Visibilit√© au d√©part',
            't_visibility_statute_mi': 'üëÅÔ∏è Visibilit√© √† l\'arriv√©e',
            
            # Features calcul√©es
            'heat_index': 'üî• Indice de chaleur',
            'wind_chill': '‚ùÑÔ∏è Refroidissement √©olien',
            'temp_diff': 'üå°Ô∏è Diff√©rence de temp√©rature d√©part-arriv√©e',
            'pressure_diff': 'üìä Diff√©rence de pression d√©part-arriv√©e',
            'wind_speed_diff': 'üå™Ô∏è Diff√©rence vitesse vent d√©part-arriv√©e',
            
            # Compagnies a√©riennes
            'airline_code=AA': '‚úàÔ∏è American Airlines',
            'airline_code=DL': '‚úàÔ∏è Delta Airlines', 
            'airline_code=UA': '‚úàÔ∏è United Airlines',
            'airline_code=WN': '‚úàÔ∏è Southwest Airlines',
            'airline_code=B6': '‚úàÔ∏è JetBlue Airways',
            'airline_code=AS': '‚úàÔ∏è Alaska Airlines',
            
            # M√©t√©o simplifi√©e
            'dep_weather_simplified=Rain': 'üåßÔ∏è Pluie au d√©part',
            'dep_weather_simplified=Snow': '‚ùÑÔ∏è Neige au d√©part',
            'dep_weather_simplified=Fog': 'üå´Ô∏è Brouillard au d√©part',
            'dep_weather_simplified=Clear': '‚òÄÔ∏è Temps clair au d√©part',
            'arr_weather_simplified=Rain': 'üåßÔ∏è Pluie √† l\'arriv√©e',
            'arr_weather_simplified=Snow': '‚ùÑÔ∏è Neige √† l\'arriv√©e',
            'arr_weather_simplified=Fog': 'üå´Ô∏è Brouillard √† l\'arriv√©e',
            'arr_weather_simplified=Clear': '‚òÄÔ∏è Temps clair √† l\'arriv√©e',
            
            # Impact m√©t√©o
            'dep_weather_impact=High': '‚ö†Ô∏è Impact m√©t√©o √©lev√© au d√©part',
            'dep_weather_impact=Medium': '‚ö° Impact m√©t√©o moyen au d√©part',
            'dep_weather_impact=Low': '‚úÖ Impact m√©t√©o faible au d√©part',
            'arr_weather_impact=High': '‚ö†Ô∏è Impact m√©t√©o √©lev√© √† l\'arriv√©e',
            'arr_weather_impact=Medium': '‚ö° Impact m√©t√©o moyen √† l\'arriv√©e',
            'arr_weather_impact=Low': '‚úÖ Impact m√©t√©o faible √† l\'arriv√©e',
            'overall_weather_impact=High': 'üö® Impact m√©t√©o global √©lev√©',
            'overall_weather_impact=Medium': '‚ö° Impact m√©t√©o global moyen',
            'overall_weather_impact=Low': '‚úÖ Impact m√©t√©o global faible',
        }
        
        print(f"\nüéØ TOP {top_n} - IMPORTANCE DES FACTEURS DE RETARD")
        print("=" * 80)
        
        for i, (_, row) in enumerate(self.feature_importance.head(top_n).iterrows(), 1):
            feature_name = row['feature']
            importance = row['importance']
            
            # Obtenir la description
            description = descriptions.get(feature_name, feature_name)
            
            # Calculer le pourcentage d'importance
            total_importance = self.feature_importance['importance'].sum()
            percentage = (importance / total_importance * 100) if total_importance > 0 else 0
            
            # Affichage format√©
            bar_length = int(importance * 50 / self.feature_importance['importance'].max())
            bar = "‚ñà" * bar_length + "‚ñí" * (50 - bar_length)
            
            print(f"{i:2d}. {description}")
            print(f"    {bar} {importance:.4f} ({percentage:.1f}%)")
            print()
    
    def _create_model(self, model_type: str, y_train: pd.Series):
        """
        Cr√©e le mod√®le de machine learning selon le type sp√©cifi√©
        
        Args:
            model_type: Type de mod√®le √† cr√©er
            y_train: Labels d'entra√Ænement pour calculer les poids
            
        Returns:
            Mod√®le initialis√©
        """
        # Calcul du ratio de d√©s√©quilibre pour les mod√®les qui le supportent
        ratio = len(y_train[y_train == 0]) / len(y_train[y_train == 1])
        class_weight_dict = {0: 1, 1: ratio}
        
        print(f"  Cr√©ation du mod√®le: {model_type}")
        print(f"  Ratio de d√©s√©quilibre: {ratio:.1f}:1")
        
        if model_type == 'decision_tree':
            return DecisionTreeClassifier(
                max_depth=10,
                min_samples_split=20,
                min_samples_leaf=10,
                class_weight='balanced',
                random_state=self.random_state
            )
            
        elif model_type == 'random_forest':
            return RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                min_samples_split=20,
                min_samples_leaf=10,
                class_weight='balanced',
                random_state=self.random_state,
                n_jobs=-1
            )
            
        elif model_type == 'logistic_regression':
            return LogisticRegression(
                class_weight='balanced',
                random_state=self.random_state,
                max_iter=1000,
                solver='liblinear'
            )
            
        elif model_type == 'svm':
            return SVC(
                kernel='rbf',
                class_weight='balanced',
                probability=True,  # Important pour predict_proba
                random_state=self.random_state,
                C=1.0
            )
            
        elif model_type == 'knn':
            return KNeighborsClassifier(
                n_neighbors=5,
                weights='distance',  # Pond√©ration par distance
                n_jobs=-1
            )
            
        elif model_type == 'lightgbm':
            if not LIGHTGBM_AVAILABLE:
                raise ValueError("LightGBM n'est pas install√©. Utilisez: pip install lightgbm")
            
            return LGBMClassifier(
                objective='binary',
                metric='binary_logloss',
                boosting_type='gbdt',
                n_estimators=100,
                max_depth=4,
                learning_rate=0.1,
                class_weight='balanced',
                random_state=self.random_state,
                verbose=-1
            )
            
        elif model_type == 'xgboost':
            return XGBClassifier(
                objective="binary:logistic",
                eval_metric=["aucpr","logloss"],
                tree_method="hist",
                n_estimators=2000,          # gros, mais on stoppe t√¥t
                learning_rate=0.05,         # plus doux
                max_depth=4,                # ‚Üì complexit√©
                min_child_weight=10,        # ‚Üë taille min des feuilles
                gamma=2,                    # p√©nalise les splits faibles
                reg_alpha=4,                # L1
                reg_lambda=6,               # L2
                subsample=0.7,              # bagging
                colsample_bytree=0.7,       # feature subsampling
                max_delta_step=2,           # stabilise updates classe rare
                scale_pos_weight=ratio,     # neg/pos sur le TRAIN COURANT
                random_state=42,
                n_jobs=-1
            )
            
        elif model_type == 'xgboost_tuned':
            return XGBClassifier(
                objective='binary:logistic',
                eval_metric='aucpr',  # Optimis√© pour les classes d√©s√©quilibr√©es
                n_estimators=300,
                max_depth=4,
                learning_rate=0.05,
                subsample=0.8,
                colsample_bytree=0.8,
                scale_pos_weight=ratio,
                reg_alpha=0.1,
                reg_lambda=0.1,
                random_state=self.random_state
            )
            
        else:
            available_models = [
                'decision_tree', 'random_forest', 'logistic_regression', 
                'svm', 'knn', 'xgboost', 'xgboost_tuned'
            ]
            if LIGHTGBM_AVAILABLE:
                available_models.append('lightgbm')
                
            raise ValueError(
                f"Type de mod√®le '{model_type}' non support√©.\n"
                f"Mod√®les disponibles: {available_models}"
            )
    
    def _optimize_threshold(self, y_true: pd.Series, y_pred_proba: np.ndarray):
        """Optimise le seuil de d√©cision bas√© sur le F1-score"""
        
        precision, recall, thresholds = precision_recall_curve(y_true, y_pred_proba)
        f1_scores = 2 * (precision * recall) / (precision + recall)
        f1_scores = np.nan_to_num(f1_scores)
        
        optimal_idx = np.argmax(f1_scores)
        self.optimal_threshold = thresholds[optimal_idx]
        
        print(f"  üéØ Seuil optimal: {self.optimal_threshold:.3f} (F1: {f1_scores[optimal_idx]:.3f})")
    
    def _calculate_risk_thresholds(self, y_true: pd.Series, y_pred_proba: np.ndarray):
        """
        Calcule automatiquement les seuils de classification de risque
        bas√©s sur la distribution des probabilit√©s
        
        M√©thode: Utilise les percentiles de la distribution pour d√©finir 3 zones √©quilibr√©es
        """
        # S√©parer les probabilit√©s selon la classe r√©elle
        probs_no_delay = y_pred_proba[y_true == 0]
        probs_delay = y_pred_proba[y_true == 1]
        
        # M√©thode 1: Point de s√©paration entre les deux distributions
        # On cherche o√π les deux distributions se chevauchent le moins
        median_no_delay = np.median(probs_no_delay)
        median_delay = np.median(probs_delay)
        
        # Seuil bas: m√©diane de la classe "pas de retard" (zone s√ªre)
        self.risk_threshold_low = median_no_delay
        
        # Seuil haut: le seuil optimal de d√©cision (d√©j√† calcul√©)
        self.risk_threshold_high = self.optimal_threshold
        
        print(f"  üìä Seuils de risque calcul√©s automatiquement:")
        print(f"     ‚Ä¢ Faible/Mod√©r√©: {self.risk_threshold_low:.3f}")
        print(f"     ‚Ä¢ Mod√©r√©/√âlev√©:  {self.risk_threshold_high:.3f}")
        print(f"     ‚Üí Bas√©s sur: m√©diane(pas retard)={median_no_delay:.3f}, m√©diane(retard)={median_delay:.3f}")
    
    def _calculate_metrics(self, y_true: pd.Series, y_pred_proba: np.ndarray, X_test: np.ndarray) -> Dict:
        """Calcule les m√©triques de performance avec d√©tection d'overfitting"""
        
        y_pred = (y_pred_proba >= self.optimal_threshold).astype(int)
        
        metrics = {
            'roc_auc': roc_auc_score(y_true, y_pred_proba),
            'pr_auc': average_precision_score(y_true, y_pred_proba),
            'f1_score': f1_score(y_true, y_pred),
            'precision': precision_score(y_true, y_pred),
            'recall': recall_score(y_true, y_pred),
            'optimal_threshold': float(self.optimal_threshold),
            'confusion_matrix': confusion_matrix(y_true, y_pred).tolist(),
            'n_test_samples': len(y_true),
            'test_class_distribution': y_true.value_counts().to_dict()
        }
        
        # Affichage des r√©sultats
        print(f"\nüìä M√âTRIQUES DE PERFORMANCE:")
        print(f"  ROC-AUC: {metrics['roc_auc']:.3f}")
        print(f"  PR-AUC: {metrics['pr_auc']:.3f}")
        print(f"  F1-Score: {metrics['f1_score']:.3f}")
        print(f"  Pr√©cision: {metrics['precision']:.3f}")
        print(f"  Rappel: {metrics['recall']:.3f}")
        
        return metrics
    
    def detect_overfitting(self, X_train: np.ndarray, y_train: pd.Series, 
                          X_test: np.ndarray, y_test: pd.Series) -> Dict:
        """
        D√©tecte l'overfitting en comparant les performances sur les donn√©es d'entra√Ænement et de test
        
        Args:
            X_train: Donn√©es d'entra√Ænement pr√©process√©es
            y_train: Labels d'entra√Ænement 
            X_test: Donn√©es de test pr√©process√©es
            y_test: Labels de test
            
        Returns:
            Dictionnaire avec les indicateurs d'overfitting
        """
        print(f"\nüîç ANALYSE DE L'OVERFITTING")
        print("=" * 60)
        
        # Pr√©dictions sur les donn√©es d'entra√Ænement et de test
        train_proba = self.model.predict_proba(X_train)[:, 1]
        test_proba = self.model.predict_proba(X_test)[:, 1]
        
        train_pred = (train_proba >= self.optimal_threshold).astype(int)
        test_pred = (test_proba >= self.optimal_threshold).astype(int)
        
        # Calcul des m√©triques sur l'entra√Ænement et le test
        train_metrics = {
            'roc_auc': roc_auc_score(y_train, train_proba),
            'pr_auc': average_precision_score(y_train, train_proba),
            'f1_score': f1_score(y_train, train_pred),
            'precision': precision_score(y_train, train_pred),
            'recall': recall_score(y_train, train_pred)
        }
        
        test_metrics = {
            'roc_auc': roc_auc_score(y_test, test_proba),
            'pr_auc': average_precision_score(y_test, test_proba),
            'f1_score': f1_score(y_test, test_pred),
            'precision': precision_score(y_test, test_pred),
            'recall': recall_score(y_test, test_pred)
        }
        
        # Calcul des √©carts (indicateurs d'overfitting)
        overfitting_indicators = {}
        metric_names = ['roc_auc', 'pr_auc', 'f1_score', 'precision', 'recall']
        
        print("üìà COMPARAISON TRAIN vs TEST:")
        print("-" * 40)
        
        for metric in metric_names:
            train_val = train_metrics[metric]
            test_val = test_metrics[metric]
            gap = train_val - test_val
            gap_percent = (gap / train_val * 100) if train_val > 0 else 0
            
            overfitting_indicators[f'{metric}_gap'] = gap
            overfitting_indicators[f'{metric}_gap_percent'] = gap_percent
            overfitting_indicators[f'train_{metric}'] = train_val
            overfitting_indicators[f'test_{metric}'] = test_val
            
            # Interpr√©tation de l'√©cart
            status = "üü¢" if abs(gap_percent) < 5 else "üü°" if abs(gap_percent) < 10 else "üî¥"
            print(f"{status} {metric.upper():>12}: Train={train_val:.3f} | Test={test_val:.3f} | √âcart={gap:+.3f} ({gap_percent:+.1f}%)")
        
        # Validation crois√©e pour une √©valuation plus robuste
        cv_scores = self._cross_validation_analysis(X_train, y_train)
        overfitting_indicators.update(cv_scores)
        
        # √âvaluation globale de l'overfitting
        avg_gap_percent = np.mean([abs(overfitting_indicators[f'{m}_gap_percent']) for m in metric_names])
        overfitting_indicators['average_gap_percent'] = avg_gap_percent
        
        print(f"\nüéØ √âVALUATION GLOBALE:")
        print(f"   √âcart moyen: {avg_gap_percent:.1f}%")
        
        if avg_gap_percent < 5:
            overfitting_status = "Excellent"
            print(f"   ‚úÖ Status: {overfitting_status} - Pas d'overfitting d√©tect√©")
        elif avg_gap_percent < 10:
            overfitting_status = "Bon"
            print(f"   üü° Status: {overfitting_status} - L√©ger overfitting, acceptable")
        elif avg_gap_percent < 20:
            overfitting_status = "Moyen"
            print(f"   üü† Status: {overfitting_status} - Overfitting mod√©r√©, √† surveiller")
        else:
            overfitting_status = "Probl√©matique"
            print(f"   üî¥ Status: {overfitting_status} - Overfitting important d√©tect√©!")
        
        overfitting_indicators['overfitting_status'] = overfitting_status
        
        # Recommandations
        self._provide_overfitting_recommendations(overfitting_indicators)
        
        return overfitting_indicators
    
    def _cross_validation_analysis(self, X: np.ndarray, y: pd.Series) -> Dict:
        """
        Effectue une validation crois√©e pour d√©tecter la stabilit√© du mod√®le
        
        Returns:
            Dictionnaire avec les r√©sultats de validation crois√©e
        """
        print(f"\nüîÑ VALIDATION CROIS√âE (K-Fold=5):")
        print("-" * 40)
        
        # Configuration de la validation crois√©e stratifi√©e
        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=self.random_state)
        
        # M√©triques √† √©valuer
        scoring_metrics = ['roc_auc', 'f1', 'precision', 'recall']
        cv_results = {}
        
        for metric in scoring_metrics:
            scores = cross_val_score(self.model, X, y, cv=cv, scoring=metric, n_jobs=-1)
            
            cv_results[f'cv_{metric}_scores'] = scores.tolist()
            cv_results[f'cv_{metric}_mean'] = float(np.mean(scores))
            cv_results[f'cv_{metric}_std'] = float(np.std(scores))
            cv_results[f'cv_{metric}_min'] = float(np.min(scores))
            cv_results[f'cv_{metric}_max'] = float(np.max(scores))
            
            # Coefficient de variation (stabilit√©)
            cv_coeff = (np.std(scores) / np.mean(scores)) * 100 if np.mean(scores) > 0 else 0
            cv_results[f'cv_{metric}_stability'] = cv_coeff
            
            # Affichage
            stability_status = "üü¢" if cv_coeff < 10 else "üü°" if cv_coeff < 20 else "üî¥"
            print(f"{stability_status} {metric.upper():>12}: {np.mean(scores):.3f} ¬±{np.std(scores):.3f} | Stabilit√©: {cv_coeff:.1f}%")
        
        return cv_results
    
    def _provide_overfitting_recommendations(self, indicators: Dict):
        """
        Fournit des recommandations bas√©es sur l'analyse d'overfitting
        """
        print(f"\nüí° RECOMMANDATIONS:")
        print("-" * 40)
        
        avg_gap = indicators['average_gap_percent']
        status = indicators['overfitting_status']
        
        if status == "Excellent":
            print("‚úÖ Votre mod√®le est bien √©quilibr√©!")
            print("   ‚Ä¢ Les performances sont stables entre train et test")
            print("   ‚Ä¢ Aucune action corrective n√©cessaire")
            
        elif status == "Bon":
            print("üü° L√©g√®re tendance √† l'overfitting, mais acceptable:")
            print("   ‚Ä¢ Surveillez les performances sur de nouvelles donn√©es")
            print("   ‚Ä¢ Consid√©rez l'arr√™t pr√©coce si disponible")
            
        elif status == "Moyen":
            print("üü† Overfitting mod√©r√© d√©tect√©. Actions recommand√©es:")
            print("   ‚Ä¢ Augmentez la r√©gularisation du mod√®le")
            print("   ‚Ä¢ R√©duisez la complexit√© (max_depth, n_estimators)")
            print("   ‚Ä¢ Augmentez la taille des donn√©es d'entra√Ænement")
            print("   ‚Ä¢ Utilisez plus de donn√©es de validation")
            
        else:  # Probl√©matique
            print("üî¥ Overfitting important! Actions urgentes:")
            print("   ‚Ä¢ R√©duisez drastiquement la complexit√© du mod√®le")
            print("   ‚Ä¢ Augmentez fortement la r√©gularisation")
            print("   ‚Ä¢ Collectez plus de donn√©es d'entra√Ænement")
            print("   ‚Ä¢ Simplifiez les features (feature selection)")
            print("   ‚Ä¢ Utilisez l'arr√™t pr√©coce avec validation stricte")
        
        # Recommandations sp√©cifiques par m√©trique
        if indicators.get('roc_auc_gap_percent', 0) > 15:
            print("   ‚ö†Ô∏è √âcart ROC-AUC important: le mod√®le surapprend les patterns")
        
        if indicators.get('f1_score_gap_percent', 0) > 15:
            print("   ‚ö†Ô∏è √âcart F1-Score important: r√©viser le seuil de d√©cision")
        
        # Recommandations pour la validation crois√©e
        high_variance_metrics = []
        for metric in ['roc_auc', 'f1', 'precision', 'recall']:
            stability = indicators.get(f'cv_{metric}_stability', 0)
            if stability > 20:
                high_variance_metrics.append(metric)
        
        if high_variance_metrics:
            print(f"   üìä Variance √©lev√©e d√©tect√©e sur: {', '.join(high_variance_metrics)}")
            print("   ‚Ä¢ Le mod√®le manque de stabilit√© - augmentez les donn√©es")

    def plot_learning_curves(self, X: np.ndarray, y: pd.Series, 
                           cv_folds: int = 5, figsize: tuple = (15, 10)) -> plt.Figure:
        """
        Trace les courbes d'apprentissage pour d√©tecter visuellement l'overfitting
        
        Args:
            X: Donn√©es d'entra√Ænement pr√©process√©es
            y: Labels d'entra√Ænement
            cv_folds: Nombre de folds pour la validation crois√©e
            figsize: Taille de la figure
            
        Returns:
            Figure matplotlib avec les courbes d'apprentissage
        """
        from sklearn.model_selection import learning_curve, validation_curve
        
        print(f"\nüìà G√âN√âRATION DES COURBES D'APPRENTISSAGE...")
        
        fig, axes = plt.subplots(2, 2, figsize=figsize)
        fig.suptitle("Analyse de l'Overfitting - Courbes d'Apprentissage", fontsize=16, fontweight='bold')
        
        # 1. Courbe d'apprentissage principale (taille d'entra√Ænement vs performance)
        train_sizes = np.linspace(0.1, 1.0, 10)
        train_sizes_abs, train_scores, val_scores = learning_curve(
            self.model, X, y, 
            train_sizes=train_sizes,
            cv=cv_folds, 
            scoring='roc_auc',
            n_jobs=-1,
            random_state=self.random_state
        )
        
        # Calcul des moyennes et √©carts-types
        train_mean = np.mean(train_scores, axis=1)
        train_std = np.std(train_scores, axis=1)
        val_mean = np.mean(val_scores, axis=1)
        val_std = np.std(val_scores, axis=1)
        
        axes[0, 0].plot(train_sizes_abs, train_mean, 'o-', color='blue', label='Score d\'entra√Ænement')
        axes[0, 0].fill_between(train_sizes_abs, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')
        axes[0, 0].plot(train_sizes_abs, val_mean, 'o-', color='red', label='Score de validation')
        axes[0, 0].fill_between(train_sizes_abs, val_mean - val_std, val_mean + val_std, alpha=0.1, color='red')
        
        axes[0, 0].set_xlabel('Taille de l\'√©chantillon d\'entra√Ænement')
        axes[0, 0].set_ylabel('Score ROC-AUC')
        axes[0, 0].set_title('Courbe d\'Apprentissage (ROC-AUC)')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # Analyse de l'√©cart
        final_gap = train_mean[-1] - val_mean[-1]
        gap_text = f"√âcart final: {final_gap:.3f}"
        gap_color = 'green' if abs(final_gap) < 0.05 else 'orange' if abs(final_gap) < 0.1 else 'red'
        axes[0, 0].text(0.02, 0.98, gap_text, transform=axes[0, 0].transAxes, 
                       verticalalignment='top', bbox=dict(boxstyle='round', facecolor=gap_color, alpha=0.3))
        
        # 2. Courbe de validation - Complexit√© du mod√®le
        if hasattr(self.model, 'n_estimators'):
            # Pour les mod√®les bas√©s sur les arbres (RandomForest, XGBoost, etc.)
            param_name = 'n_estimators'
            param_range = [50, 100, 200, 300, 500, 800]
        elif hasattr(self.model, 'max_depth'):
            param_name = 'max_depth'
            param_range = [3, 5, 7, 10, 15, 20]
        elif hasattr(self.model, 'C'):
            param_name = 'C'
            param_range = [0.01, 0.1, 1, 10, 100, 1000]
        else:
            param_name = 'n_neighbors'
            param_range = [3, 5, 7, 10, 15, 20]
        
        try:
            train_scores_val, val_scores_val = validation_curve(
                self.model, X, y, param_name=param_name, param_range=param_range,
                cv=cv_folds, scoring='roc_auc', n_jobs=-1
            )
            
            train_mean_val = np.mean(train_scores_val, axis=1)
            train_std_val = np.std(train_scores_val, axis=1)
            val_mean_val = np.mean(val_scores_val, axis=1)
            val_std_val = np.std(val_scores_val, axis=1)
            
            axes[0, 1].plot(param_range, train_mean_val, 'o-', color='blue', label='Score d\'entra√Ænement')
            axes[0, 1].fill_between(param_range, train_mean_val - train_std_val, train_mean_val + train_std_val, alpha=0.1, color='blue')
            axes[0, 1].plot(param_range, val_mean_val, 'o-', color='red', label='Score de validation')
            axes[0, 1].fill_between(param_range, val_mean_val - val_std_val, val_mean_val + val_std_val, alpha=0.1, color='red')
            
            axes[0, 1].set_xlabel(f'Param√®tre: {param_name}')
            axes[0, 1].set_ylabel('Score ROC-AUC')
            axes[0, 1].set_title(f'Courbe de Validation - {param_name}')
            axes[0, 1].legend()
            axes[0, 1].grid(True, alpha=0.3)
            
            if param_name in ['C', 'n_estimators'] and len(param_range) > 3:
                axes[0, 1].set_xscale('log')
        except Exception as e:
            axes[0, 1].text(0.5, 0.5, f'Erreur courbe de validation:\n{str(e)}', 
                           transform=axes[0, 1].transAxes, ha='center', va='center')
            axes[0, 1].set_title('Courbe de Validation - Indisponible')
        
        # 3. Histogramme des r√©sidus (pour d√©tecter le biais)
        if hasattr(self, 'last_y_true') and hasattr(self, 'last_y_pred_proba'):
            residuals = self.last_y_true - self.last_y_pred_proba
            
            axes[1, 0].hist(residuals, bins=30, alpha=0.7, color='purple', edgecolor='black')
            axes[1, 0].axvline(np.mean(residuals), color='red', linestyle='--', 
                              label=f'Moyenne: {np.mean(residuals):.3f}')
            axes[1, 0].set_xlabel('R√©sidus (R√©el - Pr√©dit)')
            axes[1, 0].set_ylabel('Fr√©quence')
            axes[1, 0].set_title('Distribution des R√©sidus')
            axes[1, 0].legend()
            axes[1, 0].grid(True, alpha=0.3)
        else:
            axes[1, 0].text(0.5, 0.5, 'R√©sidus non disponibles\n(Entra√Ænez d\'abord le mod√®le)', 
                           transform=axes[1, 0].transAxes, ha='center', va='center')
            axes[1, 0].set_title('Distribution des R√©sidus - Indisponible')
        
        # 4. √âvolution des m√©triques par fold (stabilit√©)
        try:
            cv_roc_scores = cross_val_score(self.model, X, y, cv=cv_folds, scoring='roc_auc')
            cv_f1_scores = cross_val_score(self.model, X, y, cv=cv_folds, scoring='f1')
            
            folds = range(1, cv_folds + 1)
            axes[1, 1].plot(folds, cv_roc_scores, 'o-', label='ROC-AUC', linewidth=2, markersize=8)
            axes[1, 1].plot(folds, cv_f1_scores, 's-', label='F1-Score', linewidth=2, markersize=8)
            
            # Ligne de moyenne
            axes[1, 1].axhline(np.mean(cv_roc_scores), color='blue', linestyle=':', alpha=0.7, 
                              label=f'ROC-AUC moyen: {np.mean(cv_roc_scores):.3f}')
            axes[1, 1].axhline(np.mean(cv_f1_scores), color='orange', linestyle=':', alpha=0.7, 
                              label=f'F1 moyen: {np.mean(cv_f1_scores):.3f}')
            
            axes[1, 1].set_xlabel('Fold de Validation Crois√©e')
            axes[1, 1].set_ylabel('Score')
            axes[1, 1].set_title('Stabilit√© par Fold (Validation Crois√©e)')
            axes[1, 1].legend()
            axes[1, 1].grid(True, alpha=0.3)
            axes[1, 1].set_ylim(0, 1)
            
        except Exception as e:
            axes[1, 1].text(0.5, 0.5, f'Erreur validation crois√©e:\n{str(e)}', 
                           transform=axes[1, 1].transAxes, ha='center', va='center')
            axes[1, 1].set_title('Stabilit√© par Fold - Indisponible')
        
        plt.tight_layout()
        return fig

    def comprehensive_overfitting_report(self, X: np.ndarray, y: pd.Series, 
                                       save_plots: bool = True) -> Dict:
        """
        G√©n√®re un rapport complet d'analyse d'overfitting avec graphiques
        
        Args:
            X: Donn√©es pr√©process√©es
            y: Labels
            save_plots: Si True, sauvegarde les graphiques
            
        Returns:
            Dictionnaire avec toutes les m√©triques d'overfitting
        """
        print(f"\nüî¨ RAPPORT COMPLET D'ANALYSE D'OVERFITTING")
        print("=" * 80)
        
        # Division train/test pour l'analyse
        X_train_analysis, X_test_analysis, y_train_analysis, y_test_analysis = train_test_split(
            X, y, test_size=0.3, random_state=self.random_state, stratify=y
        )
        
        # Analyse d'overfitting d√©taill√©e
        overfitting_metrics = self.detect_overfitting(
            X_train_analysis, y_train_analysis, 
            X_test_analysis, y_test_analysis
        )
        
        # G√©n√©ration des courbes d'apprentissage
        if len(X) > 1000:  # Seulement si suffisamment de donn√©es
            learning_curves_fig = self.plot_learning_curves(X, y)
            
            if save_plots:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                curves_path = self.output_dir / f"learning_curves_{timestamp}.png"
                learning_curves_fig.savefig(curves_path, dpi=300, bbox_inches='tight')
                print(f"üìä Courbes d'apprentissage sauvegard√©es: {curves_path}")
                overfitting_metrics['learning_curves_path'] = str(curves_path)
        
        # Rapport textuel d√©taill√©
        report_path = self.output_dir / f"overfitting_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        self._save_overfitting_report(overfitting_metrics, report_path)
        
        return overfitting_metrics
    
    def _save_overfitting_report(self, metrics: Dict, report_path: Path):
        """Sauvegarde un rapport textuel d√©taill√© de l'analyse d'overfitting"""
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("RAPPORT D'ANALYSE D'OVERFITTING\n")
            f.write("=" * 50 + "\n\n")
            f.write(f"Date: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\n")
            f.write(f"Mod√®le: {type(self.model).__name__}\n")
            f.write(f"Seuil de retard: {self.delay_threshold} minutes\n\n")
            
            f.write("M√âTRIQUES TRAIN vs TEST:\n")
            f.write("-" * 30 + "\n")
            for metric in ['roc_auc', 'f1_score', 'precision', 'recall']:
                train_val = metrics.get(f'train_{metric}', 0)
                test_val = metrics.get(f'test_{metric}', 0)
                gap = metrics.get(f'{metric}_gap', 0)
                gap_percent = metrics.get(f'{metric}_gap_percent', 0)
                f.write(f"{metric.upper():>12}: Train={train_val:.3f} | Test={test_val:.3f} | √âcart={gap:+.3f} ({gap_percent:+.1f}%)\n")
            
            f.write(f"\n√âCART MOYEN: {metrics.get('average_gap_percent', 0):.1f}%\n")
            f.write(f"STATUT: {metrics.get('overfitting_status', 'Inconnu')}\n\n")
            
            f.write("VALIDATION CROIS√âE:\n")
            f.write("-" * 20 + "\n")
            for metric in ['roc_auc', 'f1', 'precision', 'recall']:
                mean_val = metrics.get(f'cv_{metric}_mean', 0)
                std_val = metrics.get(f'cv_{metric}_std', 0)
                stability = metrics.get(f'cv_{metric}_stability', 0)
                f.write(f"{metric.upper():>12}: {mean_val:.3f} ¬±{std_val:.3f} | Stabilit√©: {stability:.1f}%\n")
        
        print(f"üìÑ Rapport d√©taill√© sauvegard√©: {report_path}")

    def predict(self, X: Union[pd.DataFrame, np.ndarray], 
                threshold: Optional[float] = None) -> Tuple[np.ndarray, np.ndarray]:
        """
        Effectue des pr√©dictions sur de nouvelles donn√©es
        
        Args:
            X: Donn√©es √† pr√©dire
            threshold: Seuil de d√©cision (utilise le seuil optimal si None)
            
        Returns:
            Tuple (probabilit√©s, pr√©dictions_binaires)
        """
        if self.model is None or self.preprocessor is None:
            raise ValueError("Le mod√®le doit √™tre entra√Æn√© avant de faire des pr√©dictions")
        
        if threshold is None:
            threshold = self.optimal_threshold
        
        # Preprocessing
        if isinstance(X, pd.DataFrame):
            X_processed = self.preprocessor.transform(X)
        else:
            X_processed = X
        
        # Pr√©dictions
        probabilities = self.model.predict_proba(X_processed)[:, 1]
        predictions = (probabilities >= threshold).astype(int)
        
        return probabilities, predictions
    
    def save_model(self, timestamp: Optional[str] = None) -> Dict[str, str]:
        """
        Sauvegarde le mod√®le et ses composants
        
        Returns:
            Dictionnaire avec les chemins de sauvegarde
        """
        if timestamp is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        paths = {}
        
        # Sauvegarde du mod√®le
        model_path = self.output_dir / f"flight_delay_model_{timestamp}.joblib"
        joblib.dump(self.model, model_path)
        paths['model'] = str(model_path)
        
        # Sauvegarde du preprocesseur
        preprocessor_path = self.output_dir / f"preprocessor_{timestamp}.joblib"
        joblib.dump(self.preprocessor, preprocessor_path)
        paths['preprocessor'] = str(preprocessor_path)
        
        # Sauvegarde des m√©triques
        metrics_path = self.output_dir / f"model_metrics_{timestamp}.json"
        with open(metrics_path, 'w') as f:
            json.dump(self.training_metrics, f, indent=2)
        paths['metrics'] = str(metrics_path)
        
        # Sauvegarde de l'importance des caract√©ristiques
        if self.feature_importance is not None:
            importance_path = self.output_dir / f"feature_importance_{timestamp}.csv"
            self.feature_importance.to_csv(importance_path, index=False)
            paths['feature_importance'] = str(importance_path)
        
        # Configuration de production
        config = {
            'model_path': str(model_path),
            'preprocessor_path': str(preprocessor_path),
            'optimal_threshold': float(self.optimal_threshold),
            'delay_threshold': self.delay_threshold,
            'risk_threshold_low': float(getattr(self, 'risk_threshold_low', self.optimal_threshold * 0.67)),
            'risk_threshold_high': float(getattr(self, 'risk_threshold_high', self.optimal_threshold)),
            'feature_columns': {
                'numeric': self.numeric_features,
                'categorical': self.categorical_features,
                'ordered': self.ordered_features
            },
            'training_metrics': self.training_metrics
        }
        
        config_path = self.output_dir / f"production_config_{timestamp}.json"
        with open(config_path, 'w') as f:
            json.dump(config, f, indent=2)
        paths['config'] = str(config_path)
        
        print(f"‚úÖ Mod√®le sauvegard√© dans {self.output_dir}")
        for key, path in paths.items():
            print(f"  {key}: {Path(path).name}")
        
        return paths
    
    @classmethod
    def load_model(cls, config_path: str) -> 'FlightDelayPredictor':
        """
        Charge un mod√®le sauvegard√© √† partir de sa configuration
        
        Args:
            config_path: Chemin vers le fichier de configuration
            
        Returns:
            Instance de FlightDelayPredictor avec le mod√®le charg√©
        """
        with open(config_path, 'r') as f:
            config = json.load(f)
        
        # Cr√©er une instance
        predictor = cls(
            delay_threshold=config['delay_threshold']
        )
        
        # Charger les composants
        predictor.model = joblib.load(config['model_path'])
        predictor.preprocessor = joblib.load(config['preprocessor_path'])
        predictor.optimal_threshold = config['optimal_threshold']
        predictor.training_metrics = config['training_metrics']
        
        # Charger les seuils de risque (avec fallback pour anciens mod√®les)
        predictor.risk_threshold_low = config.get('risk_threshold_low', predictor.optimal_threshold * 0.67)
        predictor.risk_threshold_high = config.get('risk_threshold_high', predictor.optimal_threshold)
        
        # Restaurer la configuration des caract√©ristiques
        predictor.numeric_features = config['feature_columns']['numeric']
        predictor.categorical_features = config['feature_columns']['categorical']
        predictor.ordered_features = config['feature_columns']['ordered']
        
        print(f"‚úÖ Mod√®le charg√© depuis {config_path}")
        return predictor
    
    def plot_performance_metrics(self, y_true: pd.Series, y_pred_proba: np.ndarray):
        """G√©n√®re les graphiques de performance du mod√®le"""
        
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
        
        # 1. Courbe ROC
        fpr, tpr, _ = roc_curve(y_true, y_pred_proba)
        ax1.plot(fpr, tpr, color='blue', lw=2, label=f'ROC (AUC = {roc_auc_score(y_true, y_pred_proba):.3f})')
        ax1.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--', label='Al√©atoire')
        ax1.set_xlabel('Taux de Faux Positifs')
        ax1.set_ylabel('Taux de Vrais Positifs')
        ax1.set_title('Courbe ROC')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 2. Courbe Precision-Recall
        precision, recall, _ = precision_recall_curve(y_true, y_pred_proba)
        baseline = y_true.mean()
        ax2.plot(recall, precision, color='blue', lw=2, label=f'PR (AUC = {average_precision_score(y_true, y_pred_proba):.3f})')
        ax2.axhline(y=baseline, color='red', linestyle='--', label=f'Baseline ({baseline:.3f})')
        ax2.set_xlabel('Rappel')
        ax2.set_ylabel('Pr√©cision')
        ax2.set_title('Courbe Precision-Recall')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # 3. Matrice de confusion
        y_pred = (y_pred_proba >= self.optimal_threshold).astype(int)
        cm = confusion_matrix(y_true, y_pred)
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax3)
        ax3.set_xlabel('Pr√©dictions')
        ax3.set_ylabel('Valeurs R√©elles')
        ax3.set_title(f'Matrice de Confusion (seuil = {self.optimal_threshold:.3f})')
        
        # 4. Distribution des probabilit√©s
        ax4.hist(y_pred_proba[y_true == 0], bins=30, alpha=0.7, label='Pas de retard', color='green', density=True)
        ax4.hist(y_pred_proba[y_true == 1], bins=30, alpha=0.7, label='Retard', color='red', density=True)
        ax4.axvline(self.optimal_threshold, color='black', linestyle='--', label=f'Seuil optimal ({self.optimal_threshold:.3f})')
        ax4.set_xlabel('Probabilit√© de Retard')
        ax4.set_ylabel('Densit√©')
        ax4.set_title('Distribution des Probabilit√©s')
        ax4.legend()
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        return fig
    
    def plot_last_performance(self):
        """
        G√©n√®re les graphiques de performance avec les derni√®res pr√©dictions
        Version simplifi√©e pour utilisation apr√®s l'entra√Ænement
        """
        if not hasattr(self, 'last_y_true') or not hasattr(self, 'last_y_pred_proba'):
            print("‚ùå Aucune pr√©diction disponible. Entra√Ænez d'abord le mod√®le.")
            return None
            
        return self.plot_performance_metrics(self.last_y_true, self.last_y_pred_proba)

    def plot_feature_importance(self, top_n: int = 15, figsize: tuple = (12, 8)):
        """
        Affiche l'analyse de l'importance des features avec graphique et tableau
        
        Args:
            top_n: Nombre de features les plus importantes √† afficher (d√©faut: 15)
            figsize: Taille de la figure (largeur, hauteur)
        """
        if self.feature_importance is None:
            print("‚ùå Aucune analyse d'importance disponible.")
            print("   L'importance des features n'est disponible que pour certains mod√®les")
            print("   (Random Forest, XGBoost, LightGBM, Decision Tree)")
            return
        
        print("=" * 60)
        print("üìä ANALYSE DE L'IMPORTANCE DES FEATURES")
        print("=" * 60)
        
        # Affichage du tableau des top features
        top_features = self.feature_importance.head(top_n)
        print(f"\nüîù TOP {top_n} DES FEATURES LES PLUS IMPORTANTES:")
        print("-" * 50)
        print(top_features.to_string(index=False, 
                                   float_format=lambda x: f"{x:.4f}"))
        
        # Cr√©ation du graphique horizontal
        plt.figure(figsize=figsize)
        
        # Graphique en barres horizontales (invers√© pour avoir le plus important en haut)
        y_pos = range(len(top_features))
        plt.barh(y_pos, top_features['importance'], 
                color='steelblue', alpha=0.7, edgecolor='navy', linewidth=0.5)
        
        # Configuration des axes
        plt.yticks(y_pos, top_features['feature'])
        plt.xlabel('Importance', fontsize=12, fontweight='bold')
        plt.ylabel('Features', fontsize=12, fontweight='bold')
        plt.title(f'Top {top_n} - Importance des Features\n({self.model.__class__.__name__})', 
                 fontsize=14, fontweight='bold', pad=20)
        
        # Inversion de l'axe Y pour avoir le plus important en haut
        plt.gca().invert_yaxis()
        
        # Ajout des valeurs sur les barres
        for i, v in enumerate(top_features['importance']):
            plt.text(v + max(top_features['importance']) * 0.01, i, 
                    f'{v:.4f}', va='center', fontweight='bold')
        
        # Am√©lioration de l'apparence
        plt.grid(axis='x', alpha=0.3)
        plt.tight_layout()
        plt.show()
        
        # Statistiques suppl√©mentaires
        total_importance = self.feature_importance['importance'].sum()
        cumulative_top = top_features['importance'].sum()
        percentage_covered = (cumulative_top / total_importance) * 100
        
        print(f"\nüìà STATISTIQUES:")
        print(f"   ‚Ä¢ Total features: {len(self.feature_importance)}")
        print(f"   ‚Ä¢ Top {top_n} couvrent {percentage_covered:.1f}% de l'importance totale")
        print(f"   ‚Ä¢ Feature la plus importante: {top_features.iloc[0]['feature']} ({top_features.iloc[0]['importance']:.4f})")
        
        return top_features

    def quick_overfitting_check(self) -> str:
        """
        V√©rification rapide du statut d'overfitting du mod√®le entra√Æn√©
        
        Returns:
            Statut d'overfitting sous forme de cha√Æne
        """
        if not hasattr(self, 'training_metrics') or 'overfitting_analysis' not in self.training_metrics:
            return "‚ùå Analyse d'overfitting non disponible. Entra√Ænez d'abord le mod√®le."
        
        analysis = self.training_metrics['overfitting_analysis']
        status = analysis.get('overfitting_status', 'Inconnu')
        avg_gap = analysis.get('average_gap_percent', 0)
        
        status_icons = {
            'Excellent': '‚úÖ',
            'Bon': 'üü°', 
            'Moyen': 'üü†',
            'Probl√©matique': 'üî¥'
        }
        
        icon = status_icons.get(status, '‚ùì')
        
        return f"{icon} Overfitting: {status} (√âcart moyen: {avg_gap:.1f}%)"
    
    def display_overfitting_summary(self):
        """
        Affiche un r√©sum√© compact de l'analyse d'overfitting
        """
        if not hasattr(self, 'training_metrics') or 'overfitting_analysis' not in self.training_metrics:
            print("‚ùå Analyse d'overfitting non disponible. Entra√Ænez d'abord le mod√®le.")
            return
        
        analysis = self.training_metrics['overfitting_analysis']
        
        print(f"\nüîç R√âSUM√â OVERFITTING")
        print("=" * 40)
        print(f"Statut: {self.quick_overfitting_check()}")
        print(f"√âcart ROC-AUC: {analysis.get('roc_auc_gap_percent', 0):+.1f}%")
        print(f"√âcart F1-Score: {analysis.get('f1_score_gap_percent', 0):+.1f}%")
        print(f"Stabilit√© CV (ROC): {analysis.get('cv_roc_auc_stability', 0):.1f}%")
        
        # Conseil rapide
        avg_gap = analysis.get('average_gap_percent', 0)
        if avg_gap < 5:
            print("üí° Conseil: Mod√®le bien √©quilibr√©, vous pouvez l'utiliser en production")
        elif avg_gap < 15:
            print("üí° Conseil: Surveillez les performances sur de nouvelles donn√©es")
        else:
            print("üí° Conseil: R√©duisez la complexit√© ou augmentez les donn√©es")

    def display_feature_importance(self, top_n: int = 15):
        """
        Alias pour plot_feature_importance (compatibilit√©)
        """
        return self.plot_feature_importance(top_n)

    def predict_from_csv(self, 
                        csv_path: str, 
                        airports_ref_path: str = "airports_ref.csv",
                        output_path: Optional[str] = None,
                        include_probability: bool = True) -> pd.DataFrame:
        """
        Pr√©dit les retards avec classification de risque √† partir d'un fichier CSV
        
        Args:
            csv_path: Chemin vers le fichier CSV √† pr√©dire
            airports_ref_path: Chemin vers le fichier de r√©f√©rence des a√©roports
            output_path: Chemin de sortie pour sauvegarder les r√©sultats (optionnel)
            include_probability: Inclure la probabilit√© num√©rique dans le r√©sultat
            
        Returns:
            DataFrame avec les pr√©dictions et classifications de risque
        """
        if self.model is None or self.preprocessor is None:
            raise ValueError("Le mod√®le doit √™tre charg√© avant de faire des pr√©dictions")
        
        print(f"üîÑ Chargement des donn√©es depuis {csv_path}...")
        
        # Charger les donn√©es brutes pour sauvegarder les IDs
        df_raw = pd.read_csv(csv_path)
        
        # Sauvegarder f_id si pr√©sent
        f_id_column = None
        if 'f_id' in df_raw.columns:
            f_id_values = df_raw['f_id'].copy()
            f_id_column = 'f_id'
        elif 'id' in df_raw.columns:
            f_id_values = df_raw['id'].copy()
            f_id_column = 'id'
        else:
            # Cr√©er un ID temporaire bas√© sur l'index
            f_id_values = pd.Series(range(len(df_raw)), name='row_id')
            f_id_column = 'row_id'
            
        print(f"‚úÖ {len(df_raw):,} lignes charg√©es, colonne d'identifiant: {f_id_column}")
        
        # Pr√©paration des donn√©es avec load_and_prepare_data (mode production, DRY!)
        print("üîÑ Pr√©paration des caract√©ristiques (mode production)...")
        df = self.load_and_prepare_data(csv_path, airports_ref_path, for_training=False)
        
        # S√©lection des colonnes pour la pr√©diction
        feature_cols = self.numeric_features + self.categorical_features + self.ordered_features
        existing_cols = [col for col in feature_cols if col in df.columns]
        
        if len(existing_cols) != len(feature_cols):
            missing_cols = set(feature_cols) - set(existing_cols)
            print(f"‚ö†Ô∏è Colonnes manquantes: {missing_cols}")
            print("   Les colonnes manquantes seront imput√©es automatiquement.")
        
        X = df[existing_cols]
        
        # Pr√©dictions
        print("üîÑ G√©n√©ration des pr√©dictions...")
        probabilities, predictions = self.predict(X)
        
        # Classification des risques
        risk_levels = self._classify_risk_levels(probabilities)
        
        # Construction du DataFrame de r√©sultats
        results = pd.DataFrame({
            f_id_column: f_id_values[:len(probabilities)],
            'prediction': predictions,
            'risk_level': risk_levels
        })
        
        if include_probability:
            results['delay_probability'] = probabilities
            
        # Ajout de statistiques descriptives
        self._print_prediction_summary(results, probabilities)
        
        # Sauvegarde si demand√©e
        if output_path:
            results.to_csv(output_path, index=False)
            print(f"‚úÖ R√©sultats sauvegard√©s dans {output_path}")
            
        return results
    
    def _classify_risk_levels(self, probabilities: np.ndarray) -> List[str]:
        """
        Classifie les probabilit√©s en niveaux de risque en utilisant les seuils
        calcul√©s automatiquement lors de l'entra√Ænement.
        
        Args:
            probabilities: Probabilit√©s de retard (0-1)
            
        Returns:
            Liste des niveaux de risque
        
        Logique de classification adaptative:
        
        Les seuils sont calcul√©s automatiquement dans _calculate_risk_thresholds():
        
        - Faible: prob < risk_threshold_low
          ‚Üí En dessous de la m√©diane des "pas de retard"
          ‚Üí Zone de confiance √©lev√©e (pas de retard attendu)
        
        - Mod√©r√©: risk_threshold_low <= prob < risk_threshold_high (optimal_threshold)
          ‚Üí Zone d'incertitude entre les deux distributions
          ‚Üí Le mod√®le h√©site, surveillance recommand√©e
        
        - √âlev√©: prob >= risk_threshold_high (optimal_threshold)
          ‚Üí Au-del√† du seuil de d√©cision optimal
          ‚Üí Retard pr√©dit avec forte confiance
        
        Cette approche s'adapte automatiquement √† chaque mod√®le entra√Æn√©
        sans valeurs en dur dans le code.
        """
        risk_levels = []
        
        # Utiliser les seuils calcul√©s automatiquement lors de l'entra√Ænement
        # Fallback sur des valeurs par d√©faut si pas encore calcul√©s (chargement de mod√®le)
        low_threshold = getattr(self, 'risk_threshold_low', self.optimal_threshold * 0.67)
        high_threshold = getattr(self, 'risk_threshold_high', self.optimal_threshold)
        
        for prob in probabilities:
            if prob < low_threshold:
                risk_levels.append("low")
            elif prob < high_threshold:
                risk_levels.append("medium")
            else:
                risk_levels.append("high")

        return risk_levels
    
    def _print_prediction_summary(self, results: pd.DataFrame, probabilities: np.ndarray):
        """Affiche un r√©sum√© des pr√©dictions"""
        
        total = len(results)
        nb_retards = (results['prediction'] == 1).sum()
        nb_pas_retards = (results['prediction'] == 0).sum()
        
        # Distribution par niveau de risque
        risk_counts = results['risk_level'].value_counts()
        
        print(f"\nüìä R√âSUM√â DES PR√âDICTIONS ({total:,} vols analys√©s):")
        print("=" * 60)
        print(f"üî¥ Retards pr√©dits: {nb_retards:,} ({nb_retards/total*100:.1f}%)")
        print(f"üü¢ Pas de retard: {nb_pas_retards:,} ({nb_pas_retards/total*100:.1f}%)")
        print()
        print("üìà DISTRIBUTION DES RISQUES:")
        for risk_level in ["Faible", "Mod√©r√©", "√âlev√©"]:
            count = risk_counts.get(risk_level, 0)
            print(f"  {risk_level:>8}: {count:>6,} vols ({count/total*100:>5.1f}%)")
        
        print(f"\nüéØ STATISTIQUES DES PROBABILIT√âS:")
        print(f"  Probabilit√© moyenne: {np.mean(probabilities):.3f}")
        print(f"  Probabilit√© m√©diane: {np.median(probabilities):.3f}")
        print(f"  Probabilit√© min/max: {np.min(probabilities):.3f} / {np.max(probabilities):.3f}")
        print(f"  Seuil de d√©cision: {self.optimal_threshold:.3f}")

    def predict_single_flight(self, flight_data: Dict) -> Dict[str, Union[str, float, int]]:
        """
        Pr√©dit le retard pour un seul vol
        
        Args:
            flight_data: Dictionnaire avec les donn√©es du vol
            
        Returns:
            Dictionnaire avec la pr√©diction et le niveau de risque
        """
        if self.model is None or self.preprocessor is None:
            raise ValueError("Le mod√®le doit √™tre charg√© avant de faire des pr√©dictions")
        
        # Convertir en DataFrame
        df = pd.DataFrame([flight_data])
        
        # Appliquer le m√™me pipeline de pr√©paration
        # Note: Cette version simplifi√©e assume que les donn√©es sont d√©j√† format√©es
        feature_cols = self.numeric_features + self.categorical_features + self.ordered_features
        existing_cols = [col for col in feature_cols if col in df.columns]
        
        X = df[existing_cols]
        
        # Faire la pr√©diction
        probability, prediction = self.predict(X)
        
        # Classer le niveau de risque
        risk_level = self._classify_risk_levels(probability)[0]
        
        return {
            'prediction': int(prediction[0]),
            'delay_probability': float(probability[0]),
            'risk_level': risk_level,
            'delay_expected': prediction[0] == 1
        }


# Exemple d'utilisation
if __name__ == "__main__":
    # Exemple d'utilisation de la classe
    predictor = FlightDelayPredictor(
        delay_threshold=15,
        sample_size=200000,  # Pour test rapide
        random_state=42
    )
    
    # Chargement et pr√©paration des donn√©es
    df = predictor.load_and_prepare_data("C:/Temp/data-all 2025-11-04.csv", "utils/airports_ref.csv")
    
    # Entra√Ænement
    metrics = predictor.train(df, model_type='xgboost_tuned')
    
    # Sauvegarde
    paths = predictor.save_model()
    
    print("‚úÖ Classe FlightDelayPredictor pr√™te √† l'utilisation!")